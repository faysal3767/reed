{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic and advanced modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These line is required to save as csv with today's date\n",
    "today = datetime.today().strftime('%d_%b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply() # This line is to avoid \"RuntimeError: asyncio.run() cannot be called from a running event loop\"\n",
    "def topDiscountCourses(url,stopPage):\n",
    "    \"\"\"It scrapes any number of top discount courses (most popular) depending on the value of stop page.\n",
    "    If the value of stop page is 5, it scrapes top 500 discount courses. It scrapes 3000 courses, if \n",
    "    stop page value is 30. So stop page 1 corresponds to 100 courses.\n",
    "    \"\"\"\n",
    "    # To calculate the execution time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Initialize empty list of variables to be scraped\n",
    "    courseLink = []\n",
    "    courseTitle = []\n",
    "    courseProvider = []\n",
    "    subTitle = []\n",
    "    unitSold = []\n",
    "    offerPrice = []\n",
    "    originalPrice = []\n",
    "    savings = []\n",
    "    soldOrEnq = []\n",
    "    haveCpd = []\n",
    "    haveProfQual = []\n",
    "    isRegulated = []\n",
    "    awrBodyQualName = []\n",
    "    awrBodyName = []\n",
    "    cpdAccreditedBy = []\n",
    "    \n",
    "    # Assign the required stop page depending on the no of courses to be scraped\n",
    "    stopPage = stopPage\n",
    "    \n",
    "    # This portion scrapes all the indvidual courses link \n",
    "    async def fetch(session, providerUrl):\n",
    "        async with session.get(providerUrl) as response:\n",
    "            return await response.text()\n",
    "        \n",
    "    async def main():\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # Iterate over the stop page to scrape all the indvidual courses link in most popular basis with page size 100\n",
    "            for page in range(1,stopPage+1):\n",
    "                htmlCourseLink = await fetch(session, url + f\"?pageno={page}&sortby=MostPopular&pagesize=100\")\n",
    "                soupCourseLink = BeautifulSoup(htmlCourseLink,\"html.parser\")\n",
    "                # Scrapes individual course link\n",
    "                for lnk in soupCourseLink.find_all(\"h2\",class_=\"mt-4 mt-sm-1 mr-5 mb-0\"):\n",
    "                    # Need to create absolute url to make request to each individual course\n",
    "                    courseLink.append(str(\"https://www.reed.co.uk\")+lnk.find(\"a\").get(\"href\"))\n",
    "                    \n",
    "    asyncio.run(main())\n",
    "    \n",
    "    \n",
    "    # Information parsing. Scrape all the variables except courseLink by sending requests to courseLink\n",
    "    async def fetch(session, url):\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "        \n",
    "        \n",
    "    async def main():\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # Count the no of requests\n",
    "            req = 0\n",
    "            # Sending requests to each course link to extract required variables\n",
    "            for lnk,reqCount in zip(courseLink,np.arange(1,len(courseLink)+1)):\n",
    "                htmlInfo = await fetch(session, lnk)\n",
    "                req = req+1\n",
    "                print(f\"Most Popular Discount Courses => Requests Completed: {req} out of {len(courseLink)}\")\n",
    "                soupInfo = BeautifulSoup(htmlInfo,\"html.parser\")\n",
    "                # Clear all the outputs except the current one in notebook console\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                 # Extract title\n",
    "                try:\n",
    "                    titleTag = soupInfo.find(\"h1\")\n",
    "                    courseTitle.append(titleTag.text)\n",
    "                except:\n",
    "                    courseTitle.append(\"missing\")\n",
    "                \n",
    "                # Extract subtitle\n",
    "                try:\n",
    "                    subTitleTag = soupInfo.find(\"h2\")\n",
    "                    subTitle.append(subTitleTag.text)\n",
    "                except:\n",
    "                    subTitle.append(\"missing\")\n",
    "\n",
    "                \n",
    "                # Extract offer price\n",
    "                try:\n",
    "                    offerPriceTag = soupInfo.find(\"span\",class_=\"current-price\")\n",
    "                    offerPrice.append(offerPriceTag.text)\n",
    "                except:\n",
    "                    offerPrice.append(\"missing\")\n",
    "                \n",
    "                # Extract unit sale\n",
    "                try:\n",
    "                    unitSoldTag = soupInfo.find_all('strong')[1]\n",
    "                    unitSold.append(unitSoldTag.text)\n",
    "                except:\n",
    "                    unitSold.append(\"missing\")\n",
    "                \n",
    "                \n",
    "                # Extract original price\n",
    "                try:\n",
    "                    originalPriceTag = soupInfo.find(\"small\",class_=\"vat-status\")\n",
    "                    originalPrice.append(originalPriceTag.text)\n",
    "                except:\n",
    "                    originalPrice.append(\"missing\")\n",
    "                \n",
    "                # Extract providers\n",
    "                try:\n",
    "                    providerTag = soupInfo.find(\"section\",class_=\"sidebar-actions\").find(\"a\",class_=\"provider-link\")\n",
    "                    courseProvider.append(providerTag.text)\n",
    "                except:\n",
    "                    courseProvider.append(\"missing\")\n",
    "                \n",
    "                # Extract savings\n",
    "                try:\n",
    "                    savingsTag = soupInfo.find(\"span\",class_=\"icon-savings-tag price-saving\")\n",
    "                    savings.append(savingsTag.text)\n",
    "                except:\n",
    "                    savings.append(\"missing\")\n",
    "                \n",
    "                # Extract if the course is sold or enquired\n",
    "                try:\n",
    "                    soldOrEnqTag = soupInfo.find_all(\"div\",class_=\"summary-content\")[-1]\n",
    "                    soldOrEnq.append(soldOrEnqTag.text.strip())\n",
    "                except:\n",
    "                    soldOrEnq.append(\"missing\")\n",
    "                    \n",
    "                # Does the course have cpd?\n",
    "                try:\n",
    "                    cpdTag = soupInfo.find(\"div\", class_=\"badge badge-dark badge-cpd mt-2\")\n",
    "                    haveCpd.append(\"yes\" if cpdTag else \"no\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "                \n",
    "                # Does the course have professional qualification?\n",
    "                try:\n",
    "                    profQualTag = soupInfo.find(\"div\", class_=\"badge badge-dark badge-professional mt-2\")\n",
    "                    haveProfQual.append(\"yes\" if profQualTag else \"no\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Is the course qualification regulated?\n",
    "                try:\n",
    "                    reguTag = soupInfo.find(\"div\", class_=\"badge badge-dark badge-regulated mt-2\")\n",
    "                    isRegulated.append(\"yes\" if reguTag else \"no\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Extract awarding body qualification name if any\n",
    "                try:\n",
    "                    awrBodyQualNameTag = soupInfo.find(\"div\",class_=\"col\")\n",
    "                    awrBodyQualName.append(awrBodyQualNameTag.find_all(\"div\")[0].text.strip() if awrBodyQualNameTag else \"missing\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Extract awarding body name if any\n",
    "                try:\n",
    "                    awrBodyNameTag = soupInfo.find(\"div\",class_=\"col\")\n",
    "                    awrBodyName.append(awrBodyNameTag.find_all(\"div\")[1].text.strip() if awrBodyNameTag else \"missing\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Who provides cpd accreditation?\n",
    "                try:\n",
    "                    cpdAccreditedByTag = soupInfo.find(\"div\",class_=\"col\").find_all(\"div\")[-1]\n",
    "                    cpdAccreditedBy.append(cpdAccreditedByTag.text.strip().split(\"by\")[-1].strip())\n",
    "                except:\n",
    "                    cpdAccreditedBy.append(\"missing\")\n",
    "\n",
    "    asyncio.run(main())\n",
    "    \n",
    "    # Now all the variables extraction is done. Create a primary dataframe off those extracted variables\n",
    "    primaryDf = pd.DataFrame({\n",
    "        \"courseTitle\":courseTitle,\n",
    "        \"courseLink\": courseLink,\n",
    "        \"subTitle\":subTitle,\n",
    "        \"courseProvider\":courseProvider,\n",
    "        \"offerPrice\":offerPrice,\n",
    "        \"originalPrice\":originalPrice,\n",
    "        \"savings\":savings,\n",
    "        \"unitSold\":unitSold,\n",
    "        \"soldOrEnq\":soldOrEnq,\n",
    "        \"haveCpd\":haveCpd,\n",
    "        \"cpdAccreditedBy\":cpdAccreditedBy,\n",
    "        \"haveProfQual\":haveProfQual,\n",
    "        \"isRegulated\":isRegulated,\n",
    "        #\"awrBodyQualName\":awrBodyQualName,\n",
    "        #\"awrBodyName\":awrBodyName\n",
    "    })\n",
    "    \n",
    "    \"\"\"Data processing, cleaning, and feature engineering.\n",
    "    We will create a new dataframe for this purpose.\n",
    "    Because we do not want to mutate the original dataframe\"\"\"\n",
    "    # Create an empty dataframe to store cleaned data\n",
    "    cleanedDf = pd.DataFrame()\n",
    "    # Extract id from courseLink\n",
    "    cleanedDf.insert(loc=0, value=primaryDf.courseLink.str.split(\"/\").str.get(5).str.replace(\"#\",\"\"),column=\"courseId\")\n",
    "    \n",
    "    # Clean original price. Remove text from digits\n",
    "    removeStringsFromOriginalPrice = primaryDf.originalPrice.apply(lambda x: \"\".join(re.findall(r\"[0-9\\.]\",x)))\n",
    "    # Convert empty strings to nan and fill nan with 0\n",
    "    cleanedDf[\"originalPrice\"] = pd.to_numeric(removeStringsFromOriginalPrice,errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    # Clean offer price\n",
    "    removeCommaAndPoundFromOfferPrice = primaryDf.offerPrice.str.replace(\"£\",\"\").str.replace(\",\",\"\")\n",
    "    # Convert any strings (FREE) to nan and fill nan with 0\n",
    "    cleanedDf[\"offerPrice\"] = pd.to_numeric(removeCommaAndPoundFromOfferPrice, errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    # Clean savings. Keep the digits only\n",
    "    removeStringsFromSavings = primaryDf.savings.apply(lambda x: \"\".join(re.findall(r\"[0-9\\.]\",x)))\n",
    "    # Convert empty strings to nan and fill nan with 0 and cast into integer\n",
    "    cleanedDf[\"savings(%)\"] = pd.to_numeric(removeStringsFromSavings, errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "    # Clean awarding body name\n",
    "    #cleanedDf[\"awrBodyName\"] = primaryDf.awrBodyName.str.split(\"\\n\").str.get(1).fillna(\"missing\")\n",
    "    \n",
    "    # Remove comma from unitSold.\n",
    "    removeCommaFromUnitSold = primaryDf.unitSold.str.replace(\",\",\"\")\n",
    "    # Also cast non digits to nan and then replace nan with 0, and cast into integer\n",
    "    cleanedDf[\"unitSold\"] = pd.to_numeric(removeCommaFromUnitSold, errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "    # Extract course level from awarding body qualification name and fill nan with missing\n",
    "    #cleanedDf[\"courseLevel\"] = pd.to_numeric(primaryDf.awrBodyQualName.str.split(\" \").str.get(1), errors=\"coerce\").fillna(\"missing\")\n",
    "    \n",
    "    # Create the finalDF with required variables from primaryDf and cleanedDf\n",
    "    finalDf = pd.DataFrame({\n",
    "        \"courseId\":cleanedDf.courseId,\n",
    "        \"courseTitle\":primaryDf.courseTitle,\n",
    "        \"subTitle\":primaryDf.subTitle,\n",
    "        \"courseLink\":primaryDf.courseLink,\n",
    "        \"courseProvider\":primaryDf.courseProvider,\n",
    "        \"unitSold\":cleanedDf.unitSold,\n",
    "        \"soldOrEnq\":primaryDf.soldOrEnq,\n",
    "        \"offerPrice\":cleanedDf.offerPrice,\n",
    "        \"originalPrice\":cleanedDf.originalPrice,\n",
    "        \"haveCpd\":primaryDf.haveCpd,\n",
    "        \"cpdAccreditedBy\":cpdAccreditedBy,\n",
    "        \"haveProfQual\":primaryDf.haveProfQual,\n",
    "        \"isRegulated\":primaryDf.isRegulated,\n",
    "        #\"awrBodyName\":cleanedDf.awrBodyName,\n",
    "        #\"awrBodyQualName\":primaryDf.awrBodyQualName,\n",
    "        #\"courseLevel\": cleanedDf.courseLevel\n",
    "    })\n",
    "    \n",
    "    # Calculate the program execution time in mins \n",
    "    endTime = time.time()\n",
    "    durationInMins = np.round((endTime-startTime)/60,2)\n",
    "    print(f\"Most Popular {finalDf.shape[0]} Discount Courses => Time Required to Scrape {finalDf.shape[0]} Records => {durationInMins} Minutes\")\n",
    "    # Write the file as csv with today's date\n",
    "    return finalDf.to_csv(f\"{today}_top_{finalDf.shape[0]}_courses.csv\",index=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Popular 500 Discount Courses => Time Required to Scrape 500 Records => 4.01 Minutes\n"
     ]
    }
   ],
   "source": [
    "# We need to scrape top 500 discount courses\n",
    "topDiscountCourses(\"https://www.reed.co.uk/courses/discount\",5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
